

## 1. Structure the Data (Why we start here)

**Problem we’re solving:**
Right now each referral is a flat record — child, maybe a relative, maybe a perpetrator — but our business question is longitudinal: *“what happened to this same child later?”* To answer that, we first have to make the data “matchable.”

**What we do:**

1. **Standardize identities**

   * Clean first/last name, normalize DOB, scrub SSN.
   * **Why:** a large portion of PA referrals have partial or dirty identifiers; if we don’t normalize, the matching rules from OCYF will under-fire and we’ll undercount subsequent referrals.

2. **Split into logical tables**

   * **Child-on-referral** (1 row per child per referral)
   * **Relatives/household** (who else is on the referral)
   * **Perpetrators** (optional secondary match)
   * **Why:** OCYF’s rules for “Likely” and “Need Info” explicitly say *“only include if you can identify a common parent/guardian/sibling.”* That’s impossible to check in a single wide table at scale; separating relatives lets us look them up only when the rule needs it.

3. **Create blocking keys** (SSN, LastName+BirthYear, etc.)

   * **Why:** we have ~7M rows. We cannot compare everyone to everyone. Blocking gives us “small neighborhoods” of likely matches and keeps the logic fast **without weakening the rules**.

**How this helps the project:**
This step makes it possible to apply the *same* match definitions DHS put in the slides, but on 7M records, not on a small sample.

---

## 2. Cascade Matching (Applying the DHS logic as-is)

**Problem we’re solving:**
The slides define 5 match strengths (Match, Likely, Need Info, Research, No/Unlikely). If we apply the weakest ones too early, we’ll over-match and artificially inflate “subsequent referrals.” If we apply the strongest ones first, we get a reliable backbone we can safely add to.

**What we do (in order):**

1. **Pass 1 – Definite Matches**

   * Apply every row from **“Match Type – Match”**.
   * These are high-evidence patterns: strong name + SSN, or name + DOB + SSN, or LN + DOB + SSN.
   * **Why:** this gives us a core set of referrals we are confident belong to the *same child*. This is the foundation of the longitudinal dataset.

2. **Pass 2 – Likely Matches (with family check)**

   * Apply patterns from **“Match Type – Likely Match.”**
   * The slide says: *“Include as a match only if you can identify a common parent, guardian, or sibling…”*
   * So we look up the relatives table to confirm there is actually a shared household/family link.
   * **Why:** this lets us keep OCYF’s flexibility (because perpetrators / household members don’t always have perfect identifiers) **without** polluting the data with false positives.

3. **Pass 3 – Need Info / Research (conditional)**

   * Slides: *“Only include referrals under these two match types if there is nothing else but an exact SSN match that has an exact match family member or household member…”*
   * We do exactly that: we keep only the ones where SSN is doing the work **and** we can prove the family bridge.
   * **Why:** this preserves the business intent — “we trust an SSN if we can see it belongs to the same family system” — and drops all the low-signal rows.

4. **Final Exclusion – No Match / Unlikely**

   * First slide said: *“Exclude all scenarios under the ‘Unlikely Match’ and ‘No Match’ match types.”*
   * **Why:** this is our guardrail; it protects the analysis from arguments later like “maybe these were the same kid.”

**How this helps the project:**
We end up with a set of links between referrals that is **business-valid** (mirrors the DHS tables), **auditable** (we can show which rule fired), and **scalable** (because we ran it in passes and within blocks).

---

## 3. Build Child Clusters (turn links into “this is the same child”)

**Problem we’re solving:**
The business questions (“how many later CPS?”, “how fast?”, “does this subcategory escalate?”) are asked at the *child* level, but our source data is at the *referral* level.

**What we do:**

1. **Treat every referral-child as a node.**
2. **For every accepted match from the cascade, connect the two nodes.**
3. **Run a connected-components / union–find step** to group all connected nodes together.
4. **Assign a single `child_uid`** to every referral in that group.

**Why:**

* Sometimes referral A matches referral B, and B matches C, but A doesn’t directly match C (because of missing DOB, etc.). The connected-components step respects the *spirit* of the DHS logic (“these are all the same kid across time”), not just the immediate pairwise rules.
* This gives us a stable key (`child_uid`) we can reuse in any downstream model or report.

**How this helps the project:**
Now we can say: **“for this child, the first time we saw them was a GPS → did it ever become CPS?”** — which is exactly what the IGWG wants to know.

---

## 4. Create the Longitudinal Dataset

**Problem we’re solving:**
DHS wants to know if **some GPS subcategories** (starting with “Child Sexually Acting Out”) **lead to more serious future incidents.** That is a timeline question, not a single-referral question.

**What we do:**

1. **Order all referrals per `child_uid` by date.**

   * Gives us referral sequence: 1st, 2nd, 3rd…

2. **Identify the index referral**

   * For this analysis, index = first referral for the child where `Subcategory of Abuse = 'Child Sexually Acting Out'`.
   * **Why:** the slides say Phase 1 will create the longitudinal dataset “for one GPS sub-category” first, to validate the approach.

3. **Calculate time-based measures**

   * Days from index → first subsequent GPS
   * Days from index → first CPS
   * Count of subsequent GPS
   * Count of subsequent CPS
   * Same vs different subcategory
   * **Why:** these are the exact numbers we need to answer “how often” and “how quickly” escalation happens.

4. **Produce two views**

   * **Event-level** (child × referral) → good for modeling and ad-hoc analysis
   * **Child-level summary** → good for the final study questions

**How this helps the project:**
This table becomes the analytic backbone for:

* “Which GPS subcategories correlate with future CPS?”
* “Do certain subcategories lead to a more severe incident over time?”

Because the match is already vetted, we can defend the counts.

---

## 5. Performance / 7M-Row Considerations

**Problem we’re solving:**
7M rows can make even correct logic unusable if we do pairwise comparisons.

**What we do:**

1. **Block first** (by SSN, then by LastName+BirthYear) → compare only inside blocks.
2. **Cascade** (Match → Likely → Need Info) → expensive checks only hit a small candidate set.
3. **Separate relatives** → family/household lookups are done only for candidates that need it.
4. **Materialize after each pass** → no blow-up in memory.
5. **Use connected components** → linear-ish time even at 7M.

**How this helps the project:**
We preserve the *full* DHS match logic — we don’t water it down to make it run — but we execute it in an order that’s realistic for statewide data volumes.
